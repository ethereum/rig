---
title: "Pyrmont client comparison"
author:
- name: Barnab√© Monnot
  url: https://twitter.com/barnabemonnot
  affiliation: Robust Incentives Group, Ethereum Foundation
  affiliation_url: https://github.com/ethereum/rig
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
description: |
  Onwards!
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(rmarkdown)
library(infer)

source(here::here("notebooks/lib.R"))

options(digits=10)
options(scipen = 999) 

# Make the plots a bit less pixellated
knitr::opts_chunk$set(dpi = 300)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

# A minimal theme I like
newtheme <- theme_grey() + theme(
  axis.text = element_text(size = 9),
  axis.title = element_text(size = 12),
  axis.line = element_line(colour = "#000000"),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.background = element_blank(),
  legend.title = element_text(size = 12),
  legend.text = element_text(size = 10),
  legend.box.background = element_blank(),
  legend.key = element_blank(),
  strip.text.x = element_text(size = 10),
  strip.background = element_rect(fill = "white")
)
theme_set(newtheme)

myred <- "#F05431"
myyellow <- "#FED152"
mygreen <- "#BFCE80"
client_colours <- c("#000011", "#ff9a02", "#eb4a9b", "#7dc19e")

end_epoch <- 2600
slots_per_epoch <- 32
until_slot <- (end_epoch + 2) * slots_per_epoch - 1
slot_chunk_res <- until_slot %/% 15
slots_per_year <- 365.25 * 24 * 60 * 60 / 12
epochs_per_year <- slots_per_year / slots_per_epoch
```

```{r cache=TRUE}
all_ats <- fread(here::here("pyrmont_data/all_ats.csv"))
all_bxs <- fread(here::here("pyrmont_data/all_bxs.csv"))
block_root_at_slot <- get_block_root_at_slot(all_bxs)
validators <- fread(here::here("pyrmont_data/initial_validators.csv"))
all_myopic_redundant_ats <- get_myopic_redundant_ats_detail(all_ats)
subset_ats <- fread(here::here("pyrmont_data/subset_ats.csv"))
val_series <- fread(here::here("pyrmont_data/val_series.csv"))
stats_per_slot <- fread(here::here("pyrmont_data/stats_per_slot.csv"))
```

```{r}
# start_epoch <- 675
# end_epoch <- 900
# 
# start_balances <- get_balances_active_validators(start_epoch)[
#     validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
#   ] %>%
#   mutate(
#     measurement_epoch = start_epoch
#   ) %>%
#   select(-time_active, -activation_epoch)
# 
# end_balances <- get_balances_active_validators(end_epoch)[
#     validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
#   ] %>%
#   mutate(
#     measurement_epoch = end_epoch
#   ) %>%
#   select(-time_active, -activation_epoch)
# 
# reward_rates <- start_balances %>%
#   inner_join(end_balances,
#              by = c("validator_index", "client", "first_digit")) %>%
#   mutate(reward_rate = (balance.y - balance.x) / balance.x * 100 * epochs_per_year / (measurement_epoch.y - measurement_epoch.x))
```

```{r}
# Differences by client
# mean_diff <- reward_rates %>% 
#   filter(client == "lighthouse" | client == "prysm") %>%
#   specify(formula = reward_rate ~ client) %>% 
#   calculate(stat = "diff in means", order = c("lighthouse", "prysm"))
# 
# null_distribution <- reward_rates %>% 
#   filter(client == "lighthouse" | client == "prysm") %>%
#   specify(formula = reward_rate ~ client) %>% 
#   hypothesize(null = "independence") %>% 
#   generate(reps = 1000, type = "permute") %>% 
#   calculate(stat = "diff in means", order = c("lighthouse", "prysm"))
# 
# null_distribution %>% 
#   get_pvalue(obs_stat = mean_diff, direction = "both")
```

```{r}
# Differences by node region
# mean_diff <- reward_rates %>%
#   mutate(node_group = if_else(first_digit <= 2, "A", "B")) %>%
#   specify(formula = reward_rate ~ node_group) %>% 
#   calculate(stat = "diff in means", order = c("A", "B"))
# 
# null_distribution <- reward_rates %>%
#   mutate(node_group = if_else(first_digit <= 2, "A", "B")) %>%
#   specify(formula = reward_rate ~ node_group) %>% 
#   hypothesize(null = "independence") %>% 
#   generate(reps = 1000, type = "permute") %>% 
#   calculate(stat = "diff in means", order = c("A", "B"))
# 
# null_distribution %>% 
#   get_pvalue(obs_stat = mean_diff, direction = "both")
```

```{r}
# pairwise.t.test(reward_rates$reward_rate, reward_rates$client, p.adjust.method = "none")
```

```{r}
# reward_rates %>%
#   group_by(client) %>%
#   summarise(reward_rate = mean(reward_rate))
```

```{r}
# reward_rates %>%
#   mutate(node_group = if_else(first_digit <= 2, "A", "B")) %>%
#   group_by(node_group) %>%
#   summarise(reward_rate = mean(reward_rate))
```

```{r}
# pairwise.t.test(reward_rates$reward_rate, reward_rates$first_digit, p.adjust.method = "none")
```

In this report, we focus on the initial set of 99,900 validators controlled by the Ethereum Foundation and the client teams. This report was compiled with data until epoch `r end_epoch` (`r get_date_from_epoch(end_epoch)`).

<aside>
All code available [here](https://github.com/ethereum/rig/blob/master/medalla-data-challenge/notebooks/pyrmont_compare.Rmd).
</aside>

## High-level insights

- All clients perform generally well, with improvements over the (still short) lifetime of Pyrmont.
- Attention should be given to epoch processing. We identify from the performance of validator duties (attesting for the correct head) that some clients are late to propose the initial block of a new epoch, and make the hypothesis that this lateness is due to high epoch processing overhead.
- Block-packing algorithms have improved considerably since Medalla.
  - Lighthouse has a significantly lower number of attestations in the blocks it produces.
  - üëç No client includes any aggregate attestation of size 1 if it is contained in a larger, also included, aggregate.
  - üëç Lighthouse and Nimbus have no subset aggregates in their blocks. Prysm and Teku have roughly the same numbers as in Medalla.
  - üëç All clients (except Nimbus) include very few myopic redundant aggregates (aggregates already contained in parent blocks).
- There seems to be a clear correlation between the hardware used to run validators and the validator performance, leading to slightly decreased rewards when the hardware is scaled down. In particular, Teku appears to be the most sensitive to smaller hardware specs, confirming its status as institutions-directed client.

## Client distribution

We have roughly equal distribution of clients in the network at genesis. The EF operates around 20% of each validator set associated with each client, while the remaining validators are maintained by the team behind the client itself.

```{r}
validators %>%
  .[, .(client, team=(if_else(team=="ef", "EF", "Other")))] %>%
  .[, .(count=.N), by=.(client, team)] %>%
  ggplot() +
  geom_col(aes(x = client, y = count, fill = team)) +
  scale_fill_manual(name = "Team", values = c(myyellow, myred)) +
  ggtitle("Distribution of clients in the dataset") +
  xlab("Declared client") +
  ylab("Count")
```

In the following, statistics are obtained over all EF- and client teams-controlled validators, unless otherwise noted. In particular, we do not inclde data from validators activated after genesis or validators who are not controlled by the EF or the client teams.

## Client performance

### Correctness by slot index

We observe a lot more incorrect head attestations when the attestation is made for the starting slot of a new epoch. We name `slot_index` the index of the slot in the epoch (from 0 to 31).

```{r}
stats_per_slot[
  , .(percent_correct_heads = sum(correct_heads) / sum(expected_ats) * 100),
  by= .(slot_index=att_slot%%32)
] %>%
  ggplot() +
  geom_col(aes(x = slot_index, y = percent_correct_heads), fill=myred) +
  xlab("Slot index") +
  ylab("Percent of correct head attestations")
```

Attesters get the head wrong whenever the block they are supposed to attest for is late, and comes much after the attestation was published. We can check which clients are producing these late blocks.

<aside>
Note that there is a similar issue with targets, since the first block of a new epoch is also likely the candidate target checkpoint.

```{r}
stats_per_slot[
  , .(percent_correct_targets = sum(correct_targets) / sum(expected_ats) * 100),
  by= .(slot_index=att_slot%%32)
] %>%
  ggplot() +
  geom_col(aes(x = slot_index, y = percent_correct_targets), fill=myred) +
  xlab("Slot index") +
  ylab("Correct targets")
```
</aside>

```{r}
stats_per_slot[
  all_bxs[
    validators[, .(validator_index, client)],
    on=c("proposer_index" = "validator_index"),
    nomatch=NULL,
    .(slot, client)
  ],
  on = c("att_slot" = "slot"),
  nomatch=NULL
][
  , .(percent_correct_heads = sum(correct_heads) / sum(expected_ats) * 100),
  by= .(slot_index=att_slot%%32, client)
] %>%
  ggplot() +
  geom_col(aes(x = slot_index, y = percent_correct_heads, fill=client)) +
  scale_fill_manual(name="Client", values=client_colours) +
  facet_wrap(vars(client)) +
  xlab("Slot index") +
  ylab("Percent of correct head attestations")
```

Since these late blocks seem to happen more often at the start of an epoch than at the end, it is quite clear that epoch processing is at fault, with some clients likely spending more time processing the epoch and unable to publish the block on time.

We can also check over time how the performance of validators on blocks at slot index 0 evolves, again plotting per client who is expected to produce the block at slot index 0.

```{r}
chunk_size <- 50
stats_per_slot[
  all_bxs[
    validators[, .(validator_index, client)],
    on=c("proposer_index" = "validator_index"),
    nomatch=NULL,
    .(slot, client)
  ],
  on = c("att_slot" = "slot"),
  nomatch=NULL
][
  att_slot%%32==0, .(percent_correct_heads = sum(correct_heads) / sum(expected_ats) * 100),
  by= .(epoch_chunk=(att_slot%/%32)%/%chunk_size, client)
] %>%
  ggplot() +
  geom_line(aes(x = epoch_chunk * chunk_size, y = percent_correct_heads, group=client, color=client)) +
  scale_color_manual(name="Client", values=client_colours) +
  xlab("Epoch") +
  ylab("Percent of correct head attestations")
```

Validators attesting on Teku-expected blocks at slot index 0 performed better at a time when the chain experienced difficulty and the number of block produced was lower, around epochs 200 to 300, which lines up with the suggested explanation of long epoch processing times.

## Attestations over time

In the plots below, we align on the y-axis validators activated at genesis. A point on the plot is coloured in green when the validator has managed to get their attestation included for the epoch given on the x-axis. Otherwise, the point is coloured in red. Note that we do not check for the correctness of the attestation, merely its presence in some block of the beacon chain.

The plots allow us to check when a particular client is experiencing issues, at which point some share of validators of that client will be unable to publish their attestations.

```{r}
get_grid_per_client <- function(val_series, selected_client) {
  val_series[client == selected_client] %>%
    mutate(validator_index = as.factor(validator_index)) %>%
    ggplot() +
    geom_tile(aes(x = epoch, y = validator_index, fill = included_ats)) +
    scale_fill_gradient(low = myred, high = mygreen, na.value = NA,
                        limits = c(0, max(val_series$included_ats)),
                        guide = FALSE) +
    scale_x_continuous(expand = c(0, 0)) +
    xlab("Epoch") +
    ylab("Validators") +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          panel.background=element_rect(fill=myred, colour=myred),
          axis.title.x = element_text(size = 6),
          axis.title.y = element_text(size = 6),
          axis.text.x = element_text(size = 6),
          strip.text = element_text(size = 7))
}

plot_grid <- function(start_epoch, end_epoch, committees = NULL) {
  l <- c("prysm", "lighthouse", "nimbus", "teku") %>%
    map(function(client) {
      get_grid_per_client(val_series, client)
    })
  
  l[["prysm"]] | l[["lighthouse"]] | l[["nimbus"]] | l[["teku"]]
}
```

### Lighthouse

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "lighthouse")
```

### Nimbus

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "nimbus")
```

### Prysm

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "prysm")
```

### Teku

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "teku")
```

## Block-packing

A block can include at most 128 aggregate attestations. How many aggregate attestations did each client include on average?

```{r}
chunk_size <- 25
all_ats %>%
  .[, .(included_ats = .N), by=slot] %>%
  merge(all_bxs[, .(slot, proposer_index)]) %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(included_ats = mean(included_ats)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = included_ats, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ylim(0, 128) +
  ggtitle("Average number of aggregates included per block") +
  xlab("Declared client") +
  ylab("Average number of aggregates")
```

Smaller blocks lead to healthier network, as long as they do not leave attestations aside. We check how each client manages redundancy in the next sections.

### Myopic redundant aggregates

Myopic redundant aggregates were already published, with the same attesting indices, in a previous block.

```{r}
chunk_size <- 25
all_bxs %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  merge(all_myopic_redundant_ats, by.x = c("slot"), by.y = c("slot"), all.x = TRUE) %>%
  setnafill("const", fill = 0, cols = c("n_myopic_redundant")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(n_myopic_redundant = mean(n_myopic_redundant)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = n_myopic_redundant, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ggtitle("Average number of myopic redundant aggregates per block") +
  xlab("Epoch") +
  ylab("Average myopic aggregates")
```

### Subset aggregates

```{r}
subset_until_slot <- 65000
```

Subset aggregates are aggregates included in a block which are fully covered by another aggregate included in the same block. Namely, when aggregate 1 has attesting indices $I$ and aggregate 2 has attesting indices $J$, aggregate 1 is a subset aggregate when $I \subset J$.

<aside>
This analysis is carried until epoch `r subset_until_slot %/% 32` (`r get_date_from_epoch(subset_until_slot %/% 32)`).
</aside>

```{r}
chunk_size <- 10
all_bxs[slot <= subset_until_slot] %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  merge(subset_ats, by.x = c("slot"), by.y = c("slot"), all.x = TRUE) %>%
  setnafill("const", fill = 0, cols = c("n_subset", "n_subset_ind", "n_weakly_clashing", "n_strongly_clashing")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(n_subset = mean(n_subset)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = n_subset, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ggtitle("Average number of subset aggregates per block") +
  xlab("Epoch") +
  ylab("Average subset aggregates")
```

Lighthouse and Nimbus both score a perfect 0.

```{r}
chunk_size <- 5
all_ats[slot <= subset_until_slot] %>%
  .[, .(included_ats = .N), by=slot] %>%
  merge(all_bxs[, .(slot, proposer_index)]) %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  merge(subset_ats, by.x = c("slot"), by.y = c("slot"), all.x = TRUE) %>%
  setnafill("const", fill = 0, cols = c("n_subset", "n_subset_ind", "n_weakly_clashing", "n_strongly_clashing")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(n_subset = mean(n_subset)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = n_subset, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ggtitle("Percentage of subset aggregates among included aggregates") +
  xlab("Epoch") +
  ylab("Percentage of subset aggregates in block")
```

## Reward rates since genesis

```{r}
get_reward_timelines <- function(start_epoch, end_epoch, step=25) {
  start_balances <- get_balances_active_validators(start_epoch)[
    validators[, .(validator_index, client, team, first_digit)], on="validator_index"
  ] %>%
    mutate(
      measurement_epoch = start_epoch
    ) %>%
    select(-time_active, -activation_epoch)
  
  seq(start_epoch+step, end_epoch+1, step) %>%
    map(function(epoch) {
      end_balances <- get_balances_active_validators(epoch)[
        validators[, .(validator_index, client, team, first_digit)], on="validator_index"
      ] %>%
        mutate(
          measurement_epoch = epoch
        ) %>%
        select(-time_active, -activation_epoch)
      
      t <- start_balances %>%
        inner_join(end_balances,
                   by = c("validator_index", "client", "team", "first_digit")) %>%
        mutate(reward_rate = (balance.y - balance.x) / balance.x * 100 * epochs_per_year / (measurement_epoch.y - measurement_epoch.x))
      rr <- t %>%
        group_by(client, first_digit, team, measurement_epoch.y) %>%
        summarise(avg_rr = mean(reward_rate), n_group = n())
      
      start_balances <<- end_balances
      return(rr)
    }) %>%
    bind_rows() %>%
    mutate(region = as_factor(first_digit))
}
```

```{r cache=TRUE, message=FALSE}
rr_series <- get_reward_timelines(1, end_epoch + 1, step=50)
```

We first look at the reward rates per client since genesis.

```{r}
rr_series %>%
  group_by(client, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client")
```

Ethereum foundation-controlled clients are hosted on AWS nodes scattered across four regions in roughly equal proportions. We look at the reward rates per region.

```{r}
rr_series %>%
  filter(team == "ef") %>%
  group_by(region, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=region, color=region)) +
  geom_line() +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per region") +
  scale_color_discrete(name = "Region")
```

Performing an omnibus test to detect significant difference between any of the four groups, we are unable to find such significance at epoch 800. Not long after, an experiment was performed which we describe in the next section. Before doing so, we investigate reward rates per client for validators controlled by the client team.

### Reward rates per client team

While we presented reward rates for all validators per client above, our results may have involved several competing effects. On the one hand, for each client, 20% of all validators are controlled by the EF. All validators controlled by the EF run on the same hardware for the first 1000 epochs or so (more on this in the next section). While this setting allows us to compare the performance of all clients in a controlled environment, we also expect the client teams behind the development of their client to have better knowledge of the hardware requirements of their software. Thus in the following we present two analyses: first the reward rates of all validators controlled by the EF, per client; second, the reward rates of validators controlled by the client teams.

#### EF-controlled validators

```{r}
rr_series %>%
  filter(team == "ef") %>%
  group_by(client, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client (EF-controlled)")
```

#### Client teams-controlled validators

```{r}
rr_series %>%
  filter(team != "ef") %>%
  group_by(client, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client (Client team-controlled)")
```

#### Comparison

```{r}
rr_series %>%
  mutate(is_ef = if_else(team == "ef", "EF", "Client team")) %>%
  group_by(client, is_ef, team, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, linetype=is_ef, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  scale_linetype_manual(name = "Team", values = c("solid", "dashed")) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client") +
  facet_wrap(vars(client))
```

## Experiment: Scaling down nodes

Around epoch 1020, nodes controlled by the EF in regions 1 and 2 were scaled down from t3.xlarge units (4 CPUs 16GB memory, with unlimited CPU burst) to m5.large units (2 CPUs, 8GB memory, no burst). We observe a significant loss of performance despite continuous uptime.

Large decreases in all plots below for regions 1 and 2 indicate when nodes were stopped and restarted, circa epochs 1000 for region 1 and epoch 1025 for region 2. When we compare the performance of validators before and after the scaling down of regions 1 and 2, we use epoch 900 as control and epoch 1300 as treatment.

```{r cache=TRUE, message=FALSE}
start_epoch <- 800
end_epoch <- 1400
rr_series <- get_reward_timelines(start_epoch, end_epoch, step=40)
```

```{r}
rr_series %>%
  filter(team == "ef") %>%
  group_by(region, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=region, color=region)) +
  geom_line() +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per region") +
  scale_color_discrete(name = "Region")
```

Reward rates per client are affected in roughly equal proportions.

```{r}
rr_series %>%
  filter(team == "ef") %>%
  group_by(client, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client")
```

We explore further the difference between clients in regions 1 and 2 and in regions 3 and 4.

```{r}
rr_series %>%
  mutate(region = if_else(region == "1" | region == "2", "Regions 1 and 2", "Regions 3 and 4")) %>%
  group_by(measurement_epoch.y, client, region) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, color=client, linetype=region)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  scale_linetype_manual(name = "Region", values = c("solid", "dashed")) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per region") +
  facet_wrap(vars(client))
```

It seems that Teku is responsible for most of the reward decrease in regions 1 and 2. Prysm registers a significant, albeit small, decrease in reward rates between the two region groups too.

### Analysis by duty

```{r}
chunk_size <- 50
```

We look at four metrics across each region:

- Percentage of included attestations.
- Percentage of correct targets among expected attestations.
- Percentage of correct heads among expected attestations.
- Average inclusion delay.

To obtain a time series, we divide the period between epoch `r start_epoch` and epoch `r end_epoch` in chunks of size `r chunk_size` epochs. For each validator, we record how many included attestations appear in the dataset (ranging between 0 and `r chunk_size` for each chunk), the number of correct targets, correct heads and its average inclusion delay. We average over all validators in the EF-controlled set, measuring metrics either per client or per region.

We start by looking at the metrics per region.

```{r, layout="l-body-outset", fig.height=2}
val_series[
  validators[node_code > 0, .(validator_index, client, region=str_c("Region ", as.character(first_digit)))], on="validator_index"
][epoch >= start_epoch & epoch < end_epoch,][
  , .(
    included_ats=sum(included_ats)/sum(expected_ats) * 10,
    correct_targets=sum(correct_targets)/sum(expected_ats) * 10,
    correct_heads=sum(correct_heads)/sum(expected_ats) * 10,
    inclusion_delay=mean(inclusion_delay, na.rm = TRUE)
  ),
  by=.(epoch_chunk=epoch%/%chunk_size, region)
] %>%
  melt(id.vars = c("epoch_chunk", "region")) %>%
  ggplot() +
  geom_line(aes(x = epoch_chunk * chunk_size, y = value, group=region, color=region)) +
  xlab("Epoch") +
  ylab("Value") +
  scale_color_discrete(name = "Region") +
  facet_wrap(vars(variable), nrow=1, scales="free_y") +
  theme(axis.text.y=element_text(size = 6),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        axis.text.x = element_text(size = 6),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 7),
        strip.text = element_text(size = 6))
```

Inclusion, target and head correctness all present insignificant differences between the two groups of regions 1 and 2 and regions 3 and 4. However, we observe an increase in the average inclusion delay, which should explain the decreased reward rates for validators in regions 1 and 2.

Teku validators log a higher inclusion delay than others after the switch to smaller containers, as well as worse performance on other duties.

```{r, layout="l-body-outset", fig.height=2}
val_series[
  validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
][epoch >= start_epoch & epoch < end_epoch,][
  , .(
    included_ats=sum(included_ats)/sum(expected_ats) * 10,
    correct_targets=sum(correct_targets)/sum(expected_ats) * 10,
    correct_heads=sum(correct_heads)/sum(expected_ats) * 10,
    inclusion_delay=mean(inclusion_delay, na.rm = TRUE)
  ),
  by=.(epoch_chunk=epoch%/%chunk_size, client)
] %>%
  melt(id.vars = c("epoch_chunk", "client")) %>%
  ggplot() +
  geom_line(aes(x = epoch_chunk * chunk_size, y = value, group=client, color=client)) +
  xlab("Epoch") +
  ylab("Value") +
  scale_color_manual(name = "Client", values = client_colours) +
  facet_wrap(vars(variable), nrow=1, scales="free_y") +
  theme(axis.text.y=element_text(size = 6),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        axis.text.x = element_text(size = 6),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 7),
        strip.text = element_text(size = 6))
```
