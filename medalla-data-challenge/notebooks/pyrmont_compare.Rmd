---
title: "Pyrmont client comparison"
author:
- name: Barnab√© Monnot
  url: https://twitter.com/barnabemonnot
  affiliation: Robust Incentives Group, Ethereum Foundation
  affiliation_url: https://github.com/ethereum/rig
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
description: |
  Onwards!
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(rmarkdown)
library(infer)

source(here::here("notebooks/lib.R"))

options(digits=10)
options(scipen = 999) 

# Make the plots a bit less pixellated
knitr::opts_chunk$set(dpi = 300)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

# A minimal theme I like
newtheme <- theme_grey() + theme(
  axis.text = element_text(size = 9),
  axis.title = element_text(size = 12),
  axis.line = element_line(colour = "#000000"),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.background = element_blank(),
  legend.title = element_text(size = 12),
  legend.text = element_text(size = 10),
  legend.box.background = element_blank(),
  legend.key = element_blank(),
  strip.text.x = element_text(size = 10),
  strip.background = element_rect(fill = "white")
)
theme_set(newtheme)

myred <- "#F05431"
myyellow <- "#FED152"
mygreen <- "#BFCE80"
client_colours <- c("#000011", "#ff9a02", "#eb4a9b", "#7dc19e")

end_epoch <- 1520
slots_per_epoch <- 32
until_slot <- (end_epoch + 2) * slots_per_epoch - 1
slot_chunk_res <- until_slot %/% 15
slots_per_year <- 365.25 * 24 * 60 * 60 / 12
epochs_per_year <- slots_per_year / slots_per_epoch
```

```{r cache=TRUE}
all_ats <- fread(here::here("pyrmont_data/all_ats.csv"))
all_bxs <- fread(here::here("pyrmont_data/all_bxs.csv"))
block_root_at_slot <- get_block_root_at_slot(all_bxs)
validators <- fread(here::here("pyrmont_data/initial_validators.csv"))
all_myopic_redundant_ats <- get_myopic_redundant_ats_detail(all_ats)
subset_ats <- fread(here::here("pyrmont_data/subset_ats.csv"))
val_series <- fread(here::here("pyrmont_data/val_series.csv"))
stats_per_slot <- fread(here::here("pyrmont_data/stats_per_slot.csv"))
```

```{r}
# start_epoch <- 675
# end_epoch <- 900
# 
# start_balances <- get_balances_active_validators(start_epoch)[
#     validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
#   ] %>%
#   mutate(
#     measurement_epoch = start_epoch
#   ) %>%
#   select(-time_active, -activation_epoch)
# 
# end_balances <- get_balances_active_validators(end_epoch)[
#     validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
#   ] %>%
#   mutate(
#     measurement_epoch = end_epoch
#   ) %>%
#   select(-time_active, -activation_epoch)
# 
# reward_rates <- start_balances %>%
#   inner_join(end_balances,
#              by = c("validator_index", "client", "first_digit")) %>%
#   mutate(reward_rate = (balance.y - balance.x) / balance.x * 100 * epochs_per_year / (measurement_epoch.y - measurement_epoch.x))
```

```{r}
# Differences by client
# mean_diff <- reward_rates %>% 
#   filter(client == "lighthouse" | client == "prysm") %>%
#   specify(formula = reward_rate ~ client) %>% 
#   calculate(stat = "diff in means", order = c("lighthouse", "prysm"))
# 
# null_distribution <- reward_rates %>% 
#   filter(client == "lighthouse" | client == "prysm") %>%
#   specify(formula = reward_rate ~ client) %>% 
#   hypothesize(null = "independence") %>% 
#   generate(reps = 1000, type = "permute") %>% 
#   calculate(stat = "diff in means", order = c("lighthouse", "prysm"))
# 
# null_distribution %>% 
#   get_pvalue(obs_stat = mean_diff, direction = "both")
```

```{r}
# Differences by node region
# mean_diff <- reward_rates %>%
#   mutate(node_group = if_else(first_digit <= 2, "A", "B")) %>%
#   specify(formula = reward_rate ~ node_group) %>% 
#   calculate(stat = "diff in means", order = c("A", "B"))
# 
# null_distribution <- reward_rates %>%
#   mutate(node_group = if_else(first_digit <= 2, "A", "B")) %>%
#   specify(formula = reward_rate ~ node_group) %>% 
#   hypothesize(null = "independence") %>% 
#   generate(reps = 1000, type = "permute") %>% 
#   calculate(stat = "diff in means", order = c("A", "B"))
# 
# null_distribution %>% 
#   get_pvalue(obs_stat = mean_diff, direction = "both")
```

```{r}
# pairwise.t.test(reward_rates$reward_rate, reward_rates$client, p.adjust.method = "none")
```

```{r}
# reward_rates %>%
#   group_by(client) %>%
#   summarise(reward_rate = mean(reward_rate))
```

```{r}
# reward_rates %>%
#   mutate(node_group = if_else(first_digit <= 2, "A", "B")) %>%
#   group_by(node_group) %>%
#   summarise(reward_rate = mean(reward_rate))
```

```{r}
# pairwise.t.test(reward_rates$reward_rate, reward_rates$first_digit, p.adjust.method = "none")
```

In this report, we focus on the initial set of 99,900 validators controlled by the Ethereum and the client teams. This report was compiled with data until epoch `r end_epoch`.

## High-level insights

- All clients perform generally well, with improvements over the (still short) lifetime of Pyrmont.
- Attention should be given to epoch processing. We identify from the performance of validator duties (attesting for the correct head) that some clients are late to propose the initial block of a new epoch, and make the hypothesis that this lateness is due to high epoch processing overhead.
- Block-packing algorithms have improved considerably since Medalla.
  - Lighthouse has a significantly lower number of attestations in the blocks it produces.
  - üëç No client includes any aggregate attestation of size 1 if it is contained in a larger, also included, aggregate.
  - üëç Lighthouse and Nimbus have no subset aggregates in their blocks. Prysm and Teku have roughly the same numbers as in Medalla.
  - üëç All clients (except Nimbus) include very few myopic redundant aggregates (aggregates already contained in parent blocks).
- There seems to be a clear correlation between the hardware used to run validators and the validator performance, leading to slightly decreased rewards when the hardware is scaled down. In particular, Teku appears to be the most sensitive to smaller hardware specs, confirming its status as institutions-directed client.

## Client distribution

We have roughly equal distribution of clients in the network at genesis. The EF operates around 20% of each validator set associated with each client, while the remaining validators are maintained by the team behind the client itself.

```{r}
validators %>%
  .[, .(client, team=(if_else(team=="ef", "EF", "Other")))] %>%
  .[, .(count=.N), by=.(client, team)] %>%
  ggplot() +
  geom_col(aes(x = client, y = count, fill = team)) +
  scale_fill_manual(name = "Team", values = c(myyellow, myred)) +
  ggtitle("Distribution of clients in the dataset") +
  xlab("Declared client") +
  ylab("Count")
```

```{r}
get_grid_per_client <- function(val_series, selected_client) {
  val_series[client == selected_client] %>%
    mutate(validator_index = as.factor(validator_index)) %>%
    ggplot() +
    geom_tile(aes(x = epoch, y = validator_index, fill = included_ats)) +
    scale_fill_gradient(low = myred, high = mygreen, na.value = NA,
                        limits = c(0, max(val_series$included_ats)),
                        guide = FALSE) +
    scale_x_continuous(expand = c(0, 0)) +
    xlab("Epoch") +
    ylab("Validators") +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          panel.background=element_rect(fill=myred, colour=myred),
          axis.title.x = element_text(size = 6),
          axis.title.y = element_text(size = 6),
          axis.text.x = element_text(size = 6),
          strip.text = element_text(size = 7))
}

plot_grid <- function(start_epoch, end_epoch, committees = NULL) {
  l <- c("prysm", "lighthouse", "nimbus", "teku") %>%
    map(function(client) {
      get_grid_per_client(val_series, client)
    })
  
  l[["prysm"]] | l[["lighthouse"]] | l[["nimbus"]] | l[["teku"]]
}
```

## Client performance

### Correctness by slot index

We observe a lot more incorrect head attestations when the attestation is made for the starting slot of a new epoch. We name `slot_index` the index of the slot in the epoch (from 0 to 31).

```{r}
stats_per_slot[
  , .(percent_correct_heads = sum(correct_heads) / sum(expected_ats) * 100),
  by= .(slot_index=att_slot%%32)
] %>%
  ggplot() +
  geom_col(aes(x = slot_index, y = percent_correct_heads), fill=myred) +
  xlab("Slot index") +
  ylab("Percent of correct head attestations")
```

Attesters get the head wrong whenever the block they are supposed to attest for is late, and comes much after the attestation was published. We can check which clients are producing these late blocks.

```{r}
stats_per_slot[
  all_bxs[
    validators[, .(validator_index, client)],
    on=c("proposer_index" = "validator_index"),
    nomatch=NULL,
    .(slot, client)
  ],
  on = c("att_slot" = "slot"),
  nomatch=NULL
][
  , .(percent_correct_heads = sum(correct_heads) / sum(expected_ats) * 100),
  by= .(slot_index=att_slot%%32, client)
] %>%
  ggplot() +
  geom_col(aes(x = slot_index, y = percent_correct_heads, fill=client)) +
  scale_fill_manual(name="Client", values=client_colours) +
  facet_wrap(vars(client)) +
  xlab("Slot index") +
  ylab("Percent of correct head attestations")
```

Since these late blocks seem to happen more often at the start of an epoch than at the end, it is quite clear that epoch processing is at fault, with some clients likely spending more time processing the epoch and unable to publish the block on time.

We can also check over time how the performance of validators on blocks at slot index 0 evolves, again plotting per client who is expected to produce the block at slot index 0.

```{r}
chunk_size <- 50
stats_per_slot[
  all_bxs[
    validators[, .(validator_index, client)],
    on=c("proposer_index" = "validator_index"),
    nomatch=NULL,
    .(slot, client)
  ],
  on = c("att_slot" = "slot"),
  nomatch=NULL
][
  att_slot%%32==0, .(percent_correct_heads = sum(correct_heads) / sum(expected_ats) * 100),
  by= .(epoch_chunk=(att_slot%/%32)%/%chunk_size, client)
] %>%
  ggplot() +
  geom_line(aes(x = epoch_chunk * chunk_size, y = percent_correct_heads, group=client, color=client)) +
  scale_color_manual(name="Client", values=client_colours) +
  xlab("Epoch") +
  ylab("Percent of correct head attestations")
```

Validators attesting on Teku-expected blocks at slot index 0 performed better at a time when the chain experienced difficulty and the number of block produced was lower, around epochs 200 to 300, which lines up with the suggested explanation of long epoch processing times.

## Attestations over time

In the plots below, we align on the y-axis validators activated at genesis. A point on the plot is coloured in green when the validator has managed to get their attestation included for the epoch given on the x-axis. Otherwise, the point is coloured in red. Note that we do not check for the correctness of the attestation, merely its presence in some block of the beacon chain.

The plots allow us to check when a particular client is experiencing issues, at which point some share of validators of that client will be unable to publish their attestations.

### Lighthouse

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "lighthouse")
```

### Nimbus

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "nimbus")
```

### Prysm

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "prysm")
```

### Teku

```{r, layout="l-screen", fig.height=2}
get_grid_per_client(val_series[
  validators[, .(validator_index, client)], on="validator_index"
], "teku")
```

## Block-packing

A block can include at most 128 aggregate attestations. How many aggregate attestations did each client include on average?

```{r}
chunk_size <- 25
all_ats %>%
  .[, .(included_ats = .N), by=slot] %>%
  merge(all_bxs[, .(slot, proposer_index)]) %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(included_ats = mean(included_ats)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = included_ats, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ylim(0, 128) +
  ggtitle("Average number of aggregates included per block") +
  xlab("Declared client") +
  ylab("Average number of aggregates")
```

Smaller blocks lead to healthier network, as long as they do not leave attestations aside. We check how each client manages redundancy in the next sections.

### Myopic redundant aggregates

Myopic redundant aggregates were already published, with the same attesting indices, in a previous block.

```{r}
chunk_size <- 25
all_bxs %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  merge(all_myopic_redundant_ats, by.x = c("slot"), by.y = c("slot"), all.x = TRUE) %>%
  setnafill("const", fill = 0, cols = c("n_myopic_redundant")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(n_myopic_redundant = mean(n_myopic_redundant)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = n_myopic_redundant, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ggtitle("Average number of myopic redundant aggregates per block") +
  xlab("Epoch") +
  ylab("Average myopic aggregates")
```

### Subset aggregates

```{r}
subset_until_slot <- 15000
```

Subset aggregates are aggregates included in a block which are fully covered by another aggregate included in the same block. Namely, when aggregate 1 has attesting indices $I$ and aggregate 2 has attesting indices $J$, aggregate 1 is a subset aggregate when $I \subset J$.

<aside>
This analysis is carried until slot `r subset_until_slot`.
</aside>

```{r}
chunk_size <- 10
all_bxs[slot <= subset_until_slot] %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  merge(subset_ats, by.x = c("slot"), by.y = c("slot"), all.x = TRUE) %>%
  setnafill("const", fill = 0, cols = c("n_subset", "n_subset_ind", "n_weakly_clashing", "n_strongly_clashing")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(n_subset = mean(n_subset)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = n_subset, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ggtitle("Average number of subset aggregates per block") +
  xlab("Epoch") +
  ylab("Average subset aggregates")
```

Lighthouse and Nimbus both score a perfect 0.

```{r}
chunk_size <- 5
all_ats[slot <= subset_until_slot] %>%
  .[, .(included_ats = .N), by=slot] %>%
  merge(all_bxs[, .(slot, proposer_index)]) %>%
  merge(validators[, .(validator_index, client)],
        by.x = c("proposer_index"), by.y = c("validator_index")) %>%
  merge(subset_ats, by.x = c("slot"), by.y = c("slot"), all.x = TRUE) %>%
  setnafill("const", fill = 0, cols = c("n_subset", "n_subset_ind", "n_weakly_clashing", "n_strongly_clashing")) %>%
  mutate(epoch_chunk = slot %/% slots_per_epoch %/% chunk_size) %>%
  group_by(epoch_chunk, client) %>%
  summarise(n_subset = mean(n_subset)) %>%
  ggplot(aes(x = epoch_chunk * chunk_size, y = n_subset, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  ggtitle("Percentage of subset aggregates among included aggregates") +
  xlab("Epoch") +
  ylab("Percentage of subset aggregates in block")
```

## Reward rates since genesis

```{r message=FALSE}
get_reward_timelines <- function(start_epoch, end_epoch, step=25) {
  start_balances <- get_balances_active_validators(start_epoch)[
    validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
  ] %>%
    mutate(
      measurement_epoch = start_epoch
    ) %>%
    select(-time_active, -activation_epoch)
  
  seq(start_epoch+step, end_epoch+1, step) %>%
    map(function(epoch) {
      end_balances <- get_balances_active_validators(epoch)[
        validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
      ] %>%
        mutate(
          measurement_epoch = epoch
        ) %>%
        select(-time_active, -activation_epoch)
      
      t <- start_balances %>%
        inner_join(end_balances,
                   by = c("validator_index", "client", "first_digit")) %>%
        mutate(reward_rate = (balance.y - balance.x) / balance.x * 100 * epochs_per_year / (measurement_epoch.y - measurement_epoch.x))
      rr <- t %>%
        group_by(client, first_digit, measurement_epoch.y) %>%
        summarise(avg_rr = mean(reward_rate), n_group = n())
      
      start_balances <<- end_balances
      return(rr)
    }) %>%
    bind_rows() %>%
    mutate(region = as_factor(first_digit))
}
```

```{r cache=TRUE, message=FALSE}
rr_series <- get_reward_timelines(1, end_epoch + 1, step=50)
```

We first look at the reward rates per client since genesis.

```{r}
rr_series %>%
  group_by(client, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client")
```

Clients are hosted on AWS nodes scattered across four regions in roughly equal proportions. We look at the reward rates per region.

```{r}
rr_series %>%
  group_by(region, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=region, color=region)) +
  geom_line() +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per region") +
  scale_color_discrete(name = "Region")
```

Performing an omnibus test to detect significant difference between any of the four groups, we are unable to find such significance.

## Experiment: Scaling down nodes

Around epoch 1020, nodes from regions 1 and 2 were scaled down from t3.xlarge units (4cpu 16GB mem, with unlimited cpu burst) to m5.large units (2cpu, 8GB mem, no burst). We observe a significant loss of performance despite continuous uptime.

Large decreases in all plots below for regions 1 and 2 indicate when nodes were stopped and restarted, circa epochs 1000 for region 1 and epoch 1025 for region 2. When we compare the performance of validators before and after the scaling down of regions 1 and 2, we use epoch 900 as control and epoch 1300 as treatment.

```{r cache=TRUE, message=FALSE}
start_epoch <- 800
rr_series <- get_reward_timelines(start_epoch, end_epoch, step=20)
```

```{r}
rr_series %>%
  group_by(region, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=region, color=region)) +
  geom_line() +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per region") +
  scale_color_discrete(name = "Region")
```

Reward rates per client are affected in roughly equal proportions.

```{r}
rr_series %>%
  group_by(client, measurement_epoch.y) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, group=client, color=client)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per client")
```

We explore further the difference between clients in regions 1 and 2 and in regions 3 and 4.

```{r}
rr_series %>%
  mutate(region = if_else(region == "1" | region == "2", "Regions 1 and 2", "Regions 3 and 4")) %>%
  group_by(measurement_epoch.y, client, region) %>%
  summarise(avg_rr = sum(avg_rr * n_group) / sum(n_group)) %>%
  ggplot(aes(x = measurement_epoch.y, y = avg_rr, color=client, linetype=region)) +
  geom_line() +
  scale_color_manual(name = "Client", values = client_colours) +
  scale_linetype_manual(name = "Region", values = c("solid", "dashed")) +
  xlab("Epoch") +
  ylab("Average reward rate") +
  ggtitle("Timeline of average rates of reward per region") +
  facet_wrap(vars(client))
```

It seems that Teku is responsible for most of the reward decrease in regions 1 and 2. Prysm registers a significant, albeit small, decrease in reward rates between the two region groups too.

### Analysis by duty

```{r}
chunk_size <- 10
```

We look at four metrics across each region:

- Percentage of included attestations.
- Percentage of correct targets among expected attestations.
- Percentage of correct heads among expected attestations.
- Average inclusion delay.

To obtain a time series, we divide the period between epoch `r start_epoch` and epoch `r end_epoch` in chunks of size `r chunk_size` epochs. For each validator, we record how many included attestations appear in the dataset (ranging between 0 and `r chunk_size` for each chunk), the number of correct targets, correct heads and its average inclusion delay. We average over all validators in the EF-controlled set, measuring metrics either per client or per region.

We start by looking at the metrics per region.

```{r, layout="l-screen", fig.height=2}
val_series[
  validators[node_code > 0, .(validator_index, client, region=as.character(first_digit))], on="validator_index"
][epoch >= start_epoch,][
  , .(
    included_ats=sum(included_ats)/sum(expected_ats) * 100,
    correct_targets=sum(correct_targets)/sum(expected_ats) * 100,
    correct_heads=sum(correct_heads)/sum(expected_ats) * 100,
    inclusion_delay=mean(inclusion_delay, na.rm = TRUE)
  ),
  by=.(epoch, region)
] %>%
  melt(id.vars = c("epoch", "region")) %>%
  ggplot() +
  geom_line(aes(x = epoch, y = value, group=region, color=region)) +
  xlab("Epoch") +
  ylab("Value") +
  scale_color_discrete(name = "Region") +
  facet_wrap(vars(variable), nrow=1, scales="free_y") +
  theme(axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6),
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        strip.text = element_text(size = 4),
        legend.title = element_text(size = 6),
        legend.text = element_text(size = 6))
```

Inclusion, target and head correctness all present insignificant differences between the two groups of regions 1 and 2 and regions 3 and 4. However, we observe an increase in the average inclusion delay, which should explain the decreased reward rates for validators in regions 1 and 2.

Teku validators log a higher inclusion delay than others after the switch to smaller containers, as well as worse performance on other duties.

```{r, layout="l-screen", fig.height=2}
val_series[
  validators[node_code > 0, .(validator_index, client, first_digit)], on="validator_index"
][epoch >= start_epoch,][
  , .(
    included_ats=sum(included_ats)/sum(expected_ats) * 100,
    correct_targets=sum(correct_targets)/sum(expected_ats) * 100,
    correct_heads=sum(correct_heads)/sum(expected_ats) * 100,
    inclusion_delay=mean(inclusion_delay, na.rm = TRUE)
  ),
  by=.(epoch, client)
] %>%
  melt(id.vars = c("epoch", "client")) %>%
  ggplot() +
  geom_line(aes(x = epoch, y = value, group=client, color=client)) +
  xlab("Epoch") +
  ylab("Value") +
  scale_color_manual(name = "Client", values = client_colours) +
  facet_wrap(vars(variable), nrow=1, scales="free_y") +
  theme(axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6),
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        strip.text = element_text(size = 4),
        legend.title = element_text(size = 6),
        legend.text = element_text(size = 6))
```
